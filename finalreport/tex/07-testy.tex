\subsection{Metodologia testowania}
Testowanie stworzonego systemu odbywało się w sposób półautomatyczny. Napisany został skrypt tester.py, który w sposób automatyczny wysyła zapytania do systemu. Pytania pobierane są z arkuszu kalkulacyjnego, w którym znajdują się także spodziewane odpowiedzi. Po skompletowaniu odpowiedzi na wszystkie zadane pytania,  uzyskane wyniki są zapisywane do wyjściowego formularza. Aby ułatwić proces oceny, do pliku wyjściowego oprócz zwróconej odpowiedzi zapisywane są także parametry systemu, treści pytań oraz spodziewane odpowiedzi pochodzące z pliku wejściowego.

Stworzenie automatycznej weryfikacji odpowiedzi jest zadaniem nietrywialnym. Ponadto trudno byłoby zagwarantować poprawność takiego narzędzia. Z tego powodu podjęta została decyzja o manualnym sposobie oceny poprawności odpowiedzi.

Wyróżnione zostały cztery rodzaje odpowiedzi:
\begin{enumerate}
	\item poprawne,
	\item częściowo poprawne - są to odpowiedzi, w których zabrakło detali (na przykład odpowiedź \textit{włócznia} na pytanie \textit{Co Otto III podarował Bolesławowi Chrobremu?}, gdzie poprawną odpowiedzią byłaby \textit{włócznia św. Maurycego}),
	\item brak odpowiedzi, 
	\item niepoprawne.
\end{enumerate}

Do oceny jakości systemu wykorzystano dwa wskaźniki: pokrycie i precyzję.
\begin{equation}
		coverage =  \frac{n - n_n}{n}
	\end{equation}
\begin{equation}
		precission = \frac{n_g}{n - n_n}
\end{equation}
gdzie $n$ to liczba zadanych pytań, $n_n$ to liczba pytań, na które nie została udzielona odpowiedź a $n_g$ to liczba pytań, na które została udzielona w pełni poprawna odpowiedź. 

\subsection{Zbiór testowy}
W~celu weryfikacji poprawności naszego rozwiązania, przygotowaliśmy zbiór danych testowych, składający się ponad 250 pytań o~ogólnej tematyce. Przygotowując pytania, staraliśmy się równomiernie podzielić zbiór pomiędzy pięć typów potencjalnej odpowiedzi. Pytania pochodziły głównie z~popularnych teleturniejów takich jak Milionerzy czy Jeden z~dziesięciu ale również z~innych popularnych quizów internetowych oraz konkursów wiedzy dla młodzieży.

\begin{figure}[h!]
    \begin{tikzpicture}
        \pie [rotate = 180]
        {20.3/OSOBA,
         19.9/MIEJSCE,
         19.6/DATA,
         20.3/WIELKOŚĆ,
         19.9/RZECZ}
    \end{tikzpicture}
    \label{fig:rozklad-typow-odpowiedzi}  
    \caption{Rozkład typów oczekiwanych odpowiedzi w~przygotowanej bazie pytań}
\end{figure}

Z~powodu charakteru pytań o~fakty, większość oczekiwanych odpowiedzi to rzeczowniki lub liczebniki. Oprócz tego, pojawią się również pytania o~cechy: przymiotniki i~przysłówki.

\begin{figure}[h!]
    \begin{tikzpicture}
        \pie [rotate = 180]
        {62.5/Rzeczowniki,
         29.8/Liczebniki,
         7.7/Przymiotniki/przysłówki}
    \end{tikzpicture}
    \label{fig:rozklad-typow-odpowiedzi2}  
    \caption{Rozkład części mowy oczekiwanych odpowiedzi w~przygotowanej bazie pytań}
\end{figure}

\subsection{Uzyskane wyniki}

\subsubsection{Testowanie różnych wyszukiwarek}
W ramach pierwszej serii eksperymentów sprawdziliśmy jak wybór wyszukiwarki wpływa na jakość odpowiedzi. Przeprowadzono testy dla wyszukiwarki \textit{Google} oraz \textit{DuckDuckGo}. Wybór wyszukiwarek nie był przypadkowy. Spośród dostępnych wyszukiwarek \textit{Google} zwraca najmniej podsumowań, natomiast \textit{DuckDuckGo} najwięcej. Z drugiej strony jakość streszczeń pozyskanych z \textit{Google} jest największa, a te otrzymane z \textit{DuckDuckGo} często odbiegają od tematu. Testy zostały przeprowadzone dla startegii \textit{stopwords}.

W tabelach \ref{tab:googleStopwords} i \ref{tab:DuckStopwords} przedstawione zostały otrzymane wyniki z podziałem na typy oczekiwanych odpowiedzi. $n_g$ to liczba poprawnych odpowiedzi, $n_{pg}$ to liczba częściowo poprawnych odpowiedzi, $n_n$ odpowiada za liczbę nieudzielonych odpowiedzi oraz $n_w$ za liczbę niepoprawnych odpowiedzi.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c| }
	
	\hline
	\textbf{typPytania} & $n_g$ &$n_{pg}$&$n_n$&$n_w$&$coverage$&$precission$  \\ \hline
	DATA&24&3&10&13&$\num{0.80}$&$\num{0.60}$ \\ \hline
	WIELKOŚĆ&9&1&5&37&$\num{0.90}$&$\num{0.19}$ \\ \hline
	MIEJSCE&16&0&7&28&$\num{0.86}$&$\num{0.36}$ \\ \hline
	RZECZ&12&2&3&34&$\num{0.94}$&$\num{0.25}$\\ \hline
	OSOBA&19&2&6&25&$\num{0.88}$&$\num{0.41}$\\ \hline
	\end{tabular}
	\caption{Podsumowanie wyników uzyskanych dla przeglądarki \textit{Google} oraz strategii \textit{stopwords}}
	
	\label{tab:googleStopwords}
	
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c| }
		
		\hline
		\textbf{typPytania} & $n_g$ &$n_{pg}$&$n_n$&$n_w$&$coverage$&$precission$  \\ \hline
		DATA&24&1&18&7&$\num{0.64}$&$\num{0.75}$ \\ \hline
		WIELKOŚĆ&5&1&10&36&$\num{0.81}$&$\num{0.12}$ \\ \hline
		MIEJSCE&12&0&18&21&$\num{0.65}$&$\num{0.36}$ \\ \hline
		RZECZ&11&1&14&25&$\num{0.73}$&$\num{0.30}$\\ \hline
		OSOBA&14&0&16&22&$\num{0.69}$&$\num{0.39}$\\ \hline
	\end{tabular}
	\caption{Podsumowanie wyników uzyskanych dla przeglądarki \textit{DuckDuckGo} oraz strategii \textit{stopwords}}
	
	\label{tab:DuckStopwords}
	
\end{table}

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c| }
		
		\hline
		\textbf{preglądarka} & $n_g$ &$n_{pg}$&$n_n$&$n_w$&$coverage$&$precission$  \\ \hline
		\textit{Google}&24&1&18&7&$\num{064}$&$\num{0.75}$ \\ \hline
		\textit{DuckDuckGo}&5&1&10&36&$\num{0.81}$&$\num{0.12}$ \\ \hline
	\end{tabular}
	\caption{Porównanie otrzymanych wyników dla przeglądarek \textit{Google} oraz \textit{DuckDuckGo}}
	
	\label{tab:porownanieWysz}
	
\end{table}

W tabeli \ref{tab:porownanieWysz} znajduje się porównanie sumarycznych wyników eksperymentów dla obu przeglądarek. Warto zwrócić uwagę na spodziewaną różnicę pomiędzy pokryciem dla obu konfiguracji. Pomimo tego, że \textit{DuckDuckGo} zwraca kilkakrotnie razy więcej podsumowań, ich jakość jest stosunkowo niska i system nie odpowiedział na około $30\%$ pytań. W porównaniu dla \textit{Google} pokrycie wynosi około $88\%$.

\subsubsection{Testowanie strategii tworzenia zapytań}